import numpy as np

def _log_summary(ep_len, ep_ret, ep_num):
		# Round decimal places for more aesthetic logging messages
		ep_len = str(round(ep_len, 2))
		ep_ret = str(round(ep_ret, 2))

		# Print logging statements
		print(flush=True)
		print(f"-------------------- Episode #{ep_num} --------------------", flush=True)
		print(f"Episodic Length: {ep_len}", flush=True)
		print(f"Episodic Return: {ep_ret}", flush=True)
		print(f"------------------------------------------------------", flush=True)
		print(flush=True)
		

def rollout(policy, env, max_timesteps_per_episode):
    deterministic = True
    successful_episodes = 0

    np.random.seed(10)  # for reproducibility

    x_min, x_max = -100, 100
    y_min, y_max = -100, 100

    # generate 100 pairs
    x = np.random.uniform(x_min, x_max, 100)
    y = np.random.uniform(y_min, y_max, 100)

    initial_locations = np.column_stack((x, y))

    # initial_locations = [(-44.280261808114005, 52.27309637446834), (-3.2889133650926112, 42.07791872858735), (-18.033935677613385, 49.05259011530589), (-48.636633198304544, 56.27537133936147), (-23.987116066313483, -14.942186011360562), (74.74818440570647, 66.64305056725507), (49.56474658501199, -45.56436543577645), (65.44068576700496, -46.01236026889927), (27.9661112910358, -53.721234402009976), (-54.059827562476244, 22.24593794046808), (52.46032322104924, 63.665394779186556), (-5.892948626584925, -43.57734611549887), (-60.51813881110982, 44.841024120820876), (-74.02849203039503, -32.63000075874306), (-43.38875163533675, -66.53902608831055), (-79.07154774319267, 2.9709736137480434), (78.55283472581485, -69.2231389841854), (-71.15600260759547, 37.91248391902742), (44.0334524726303, -69.65487350393363),
    #                       (-37.798383539074024, 6.427089335821549), (-67.36146551484757, -3.676739003450834), (50.56828725299556, -68.58846322347524), (-79.95021913575398, 60.6686579575813), (24.141821775546816, 65.96200107149917), (-65.52300199417759, 76.02361349574585), (-22.901136005012305, 30.210810973435258), (48.023765353451154, -25.038635969588455), (-50.880492109731215, -43.41587609568214), (18.219363029907214, 34.44647499727253), (9.229823535110398, -40.489640182599196), (-43.9578804193534, -51.700185335819256), (58.96922883911958, 29.00928769998228), (65.90716366134615, -24.68654951430782), (-68.27413401928663, -2.7581856477974185), (55.97764786808685, 13.878996062009065), (18.157628894230157, -50.66863538098352), (-16.464331961110986, 32.177036645731164), (15.556422153766661, 69.06731125267174), (-26.28318865619356, -71.24903017256798), (-4.08611454044879, -67.33771288657466), (78.07619666863502, -34.96749654903471), (-75.84676399224918, 55.88235554170299), (-59.036249518550896, 35.65983318308895), (20.083816815592073, 24.41148154098198), (46.87880286503956, -54.31927802529985), (36.35121819011883, 15.677965170418403), (-13.94400132275969, -0.059728200469720605), (-74.21616045207686, -23.1497009847009), (-33.43751597373888, -61.38141471312107), (-54.54196673865727, 66.53163791011218), (-1.960832670539773, -61.01176209070735), (-66.8691372362643, -1.3831680601611822), (38.2906328935868, 10.359148564786807), (-63.78366517856156, 52.05694152881702), (-73.00108099831107, -23.1867976131268), (-35.569090902389675, 4.507640951388211), (9.531031269391832, -13.858915517368246), (-36.13591313546925, 72.70431706939394), (67.26643886053972, 1.042641448370091), (-75.68748083158455, 49.77322509485316), (31.09435132006101, -5.683497485643244), (1.1117598330749558, 40.494078811451786), (-65.08768168168764, 9.574886562567826), (-71.31011464942816, -46.505315783557094), (15.251131110441577, -70.06146357081892), (51.69470803205465, 0.04925155003896009), (59.145009983504565, 33.99972426221257), (63.63054546048053, 74.84795615506675), (-52.98725968570382, 71.20127911936453), (-55.35492766434974, 18.486411054108146), (-60.6773746667616, -36.44622819250701), (24.177537945616848, -38.2682119645917), (62.54745152730035, 6.988031769349561), (-65.78842798217539, -59.583783222322054), (66.61431683592915, 14.09581583495006), 
    #                       (-34.791010582428626, 25.986525250395033), (-12.540406286711232, 75.49839989847362), (2.9369931601536337, 46.437612499283034), (-75.02819882202807, 9.788448500450272), (-72.43008837129706, -61.79106440076012), (-36.03748994382032, 27.862325118779566), (7.751384139030435, 63.97326506857635), (-16.078703162223036, 9.146394679649376), (11.817639074772131, 73.04953506548213), (11.952289265534858, -11.355644065913268), (17.125110969214546, 57.47435546179696), (-14.278120819406325, 43.91509735666884), (-58.54310699127984, -56.25295829155718), (-19.945236387225137, -47.744690749484384), (-79.98662352336878, 22.359132934787112), (21.410301195657965, -14.506135368303333), (73.38835620012571, -53.919383156194925), (14.437272501721424, -17.35938093512644), (74.95811272663073, -75.5981740535943), (-17.901373524994128, 16.962149955550103), (30.398726265149634, 78.4678373986587), (-3.950091126238604, -20.533689604155484), (68.14402804988515, -76.06518383496501), (-8.043425507850856, 12.167747905191277), (32.33990630290543, -31.99379271554939)]

    total_reward = 0
    eps_rewards = []

    for i in range(100):  # number of test episodes
        obs = env.reset()
        initial_loc = initial_locations[i]
        env.init_loc = initial_loc  # set the initial location of the agent
        #obs = env.reset()  # reset the environment
        
        goal_reached = False  # flag to check if goal is reached
        
        t = 0  # number of timesteps so far

        ep_len = 0  # episodic length
        ep_ret = 0  # episodic return

        while t < max_timesteps_per_episode:  # max number of timesteps in each episode
            t += 1

            action = policy.get_action(obs, deterministic=deterministic)

            next_obs, reward, done = env.step(action)

            ep_len += 1
            ep_ret += reward

            obs = next_obs

            if done:
                goal_reached = True
            #     break  # end the episode if done

        total_reward += ep_ret
        eps_rewards.append(ep_ret)
        yield ep_len, ep_ret

        if goal_reached:
            successful_episodes += 1

    print(f"Successful Episodes: {successful_episodes}/100", flush=True)
    print(f"Average Reward: {total_reward / len(initial_locations)}", flush=True)

def eval_policy(policy, env, max_timesteps_per_episode):
	for ep_num, (ep_len, ep_ret) in enumerate(rollout(policy, env, max_timesteps_per_episode)):
		_log_summary(ep_len=ep_len, ep_ret=ep_ret, ep_num=ep_num+1)